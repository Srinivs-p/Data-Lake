{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "song_data_path = 'data/songs/*.json'\n",
    "#song_data = spark.read.json(song_data_path)\n",
    "# extract columns to create songs table\n",
    "df = spark.read.json(song_data_path)\n",
    "df = df.distinct()\n",
    "songs_table = df.select(\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\")\n",
    "songs_table = songs_table.distinct()\n",
    "output_data = songs_table.write.partitionBy(\"year\",\"artist_id\").format(\"parquet\").save(\"Songs.parquet\")\n",
    "# extract columns to create artists table\n",
    "#artists_table = \n",
    "\n",
    "# write artists table to parquet files\n",
    "#artists_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.rmdir(\"song_data.parquet\")\n",
    "import shutil\n",
    "shutil.rmtree('Songs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "output_data = 'data/'\n",
    "artist_data_path = 'data/songs/*.json'\n",
    "artists_table = df.select(\"artist_id\",\"artist_name\",\"artist_location\",\"artist_latitude\",\"artist_longitude\")\n",
    "artists_table = artists_table.distinct()\n",
    "output_data = artists_table.write.format(\"parquet\").save(os.path.join(output_data,\"artiz.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Double, StringType as String, IntegerType as Int, DateType as date , TimestampType as Timestamp\n",
    "# get filepath to log data file\n",
    "log_data = 'data/Log_data/*.json'\n",
    "\n",
    "log_dataSchema = R([\n",
    "       Fld(\"artist\",   String()),\n",
    "   Fld(\"auth\",  String() ),\n",
    "   Fld(\"firstName\",  String() ),\n",
    "   Fld(\"gender\",  String() ),\n",
    "   Fld(\"itemInSession\",  String() ),\n",
    "   Fld(\"lastName\",   String()),\n",
    "   Fld(\"length\",  Double() ),\n",
    "   Fld(\"level\",  String() ),\n",
    "   Fld(\"location\",  String() ),\n",
    "   Fld(\"method\",  String() ),\n",
    "   Fld(\"page\",  String() ),\n",
    "   Fld(\"Rregistration\",  String() ),\n",
    "   Fld(\"sessionId\",  String() ),\n",
    "   Fld(\"song\",  String() ),\n",
    "   Fld(\"status\",  Int()),\n",
    "   Fld(\"ts\",  Timestamp()),\n",
    "   Fld(\"userAgent\",  String()),\n",
    "   Fld(\"userId\",  Int()),\n",
    "])\n",
    "\n",
    "# read log data file\n",
    "log_dataSchema = spark.read.json(log_data) \n",
    "# filter by actions for song plays\n",
    "#df =log_dataSchema.select(\"page\").dropDuplicates().sort(\"page\").show()\n",
    "df = log_dataSchema.filter(log_dataSchema.page =='NextSong')\n",
    "users = df.select('userId','firstName','lastName','gender','level').dropDuplicates().sort(\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "df = log_dataSchema.filter(log_dataSchema.page =='NextSong')\n",
    "df = df.select(\"ts\")\n",
    "df = df.toPandas()\n",
    "time_data = df['ts'].apply(pd.to_datetime)\n",
    "time_data= pd.DataFrame(time_data) \n",
    "time_data= { 'Time':time_data['ts'],'hour':time_data['ts'].dt.hour, 'day':time_data['ts'].dt.day_name(), \n",
    "                     'week of year':time_data['ts'].dt.weekofyear,'month':time_data['ts'].dt.month,\n",
    "                     'year':time_data['ts'].dt.year,'weekday':time_data['ts'].dt.weekday}\n",
    "time_df = pd.DataFrame(time_data)\n",
    "time_df\n",
    "# create datetime column from original timestamp column\n",
    "time_Schema =  R([\n",
    "       Fld(\"Time\",Timestamp()),\n",
    "    Fld(\"hour\",Int()),\n",
    "    Fld(\"day\",String()),\n",
    "    Fld(\"week_of_year\",Int()),\n",
    "    Fld(\"month\",Int()),\n",
    "    Fld(\"year\",Int()),\n",
    "    Fld(\"weekday\",Int()),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(time_df,schema=time_Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "df_filter = log_dataSchema.filter(log_dataSchema.page =='NextSong')\n",
    "df_column = df_filter.select(\"ts\")\n",
    "df = df_column.toPandas()\n",
    "time_data = df['ts'].apply(pd.to_datetime)\n",
    "time_data= pd.DataFrame(time_data) \n",
    "time_data= { 'Time':time_data['ts'],'hour':time_data['ts'].dt.hour, 'day':time_data['ts'].dt.day_name(), \n",
    "                     'week of year':time_data['ts'].dt.weekofyear,'month':time_data['ts'].dt.month,\n",
    "                     'year':time_data['ts'].dt.year,'weekday':time_data['ts'].dt.weekday}\n",
    "time_df = pd.DataFrame(time_data)\n",
    "time_Schema =  R([\n",
    "       Fld(\"Time\",Timestamp()),\n",
    "    Fld(\"hour\",Int()),\n",
    "    Fld(\"day\",String()),\n",
    "    Fld(\"week_of_year\",Int()),\n",
    "    Fld(\"month\",Int()),\n",
    "    Fld(\"year\",Int()),\n",
    "    Fld(\"weekday\",Int()),\n",
    "])\n",
    "df = spark.createDataFrame(time_df,schema=time_Schema)\n",
    "df = df.select ('Time','hour','day','week_of_year','month','year','weekday')\n",
    "tiem_table = df.write.partitionBy(\"year\",\"month\").format(\"parquet\").save(\"tiem_table.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = spark.read.parquet(\"Songs.parquet\")\n",
    "song_df = s_df.createOrReplaceTempView(\"Songs_table\")\n",
    "\n",
    "a_df = spark.read.parquet(\"artists_table.parquet\")\n",
    "artist_df = a_df.createOrReplaceTempView(\"artists_table\")\n",
    "\n",
    "path = 'data/Log_data/*.json'\n",
    "songs_data = spark.read.json(path)\n",
    "stg_df = songs_data.createOrReplaceTempView(\"stg_songs\")\n",
    "\n",
    "df =spark.sql(''' select distinct row_number() over (order by \"sg.userId\") as songplay_id,sg.ts as ts,sg.userId,sg.level,b.song_id,b.artist_id,sg.sessionId,sg.location,sg.userAgent\n",
    "              from stg_songs as sg left join \n",
    "              (\n",
    "              SELECT s.song_id,a.artist_id,a.artist_name from Songs_table as s \n",
    "              left JOIN artists_table as a ON s.artist_id = a.artist_id)as b\n",
    "              on sg.artist = b.artist_name  where sg.page = 'NextSong' \n",
    "              group by sg.ts,sg.userId,sg.level,b.song_id,b.artist_id,sg.sessionId,sg.location,sg.userAgent ''')\n",
    "df = df.toPandas()\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "t_df= pd.DataFrame({'songplay_id':df['songplay_id'],'ts':df['ts'].apply(pd.to_datetime),\n",
    "       'userId':df['userId'],'level':df['level'],\n",
    "       'song_id':df['song_id'],'artist_id':df['artist_id'],'sessionId':df['sessionId'],'location':df['location'],\n",
    "       'userAgent':df['userAgent']})\n",
    "\n",
    "df = spark.createDataFrame(t_df)\n",
    "df.select('songplay_id','ts','userId','level','song_id','artist_id','sessionId','location','userAgent')\n",
    "\n",
    "\n",
    "prat_year_and_month = (df\n",
    "    .withColumn(\"year\", year(col(\"ts\").cast(\"timestamp\")))\n",
    "    .withColumn(\"month\", month(col(\"ts\").cast(\"timestamp\"))))\n",
    "prat_year_and_month.write.partitionBy(\"year\",\"month\").format(\"parquet\").save(\"songplays_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
